{
  "hash": "16a4cb87c00db2f794c86cfd97f0c512",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 5: Temporal Predictive Analysis\"\nsubtitle: \"Healthcare Access and Equity in Pennsylvania\"\nauthor: \"Zimu DENG (Mark)\"\ndate: today\nformat: \n  html:\n    code-fold: false\n    toc: true\n    toc-location: left\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n\n# 1. Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nPhiladelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. \n\nImagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have:\n- 200 stations across Philadelphia\n- Limited trucks and staff for moving bikes\n- 2-3 hours before morning rush hour demand peaks\n- **The question:** Which stations will run out of bikes by 8:30 AM?\n\nThis lab will build predictive models that forecast bike share demand across **Philadelphia**  and **2024, Q3** (different hours) to help solve this operational problem.\n\n## Learning Objectives\n\nBy the end of this assignment, I will be able to:\n\n1. **Understand panel data structure** for space-time analysis\n2. **Create temporal lag variables** to capture demand persistence\n3. **Build multiple predictive models** with increasing complexity\n4. **Validate models temporally** (train on past, test on future)\n5. **Analyze prediction errors** in both space and time\n6. **Engineer new features** based on error patterns\n7. **Critically evaluate** when prediction errors matter most\n\n---\n\n# 2. Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"cd859f2a38103dda55948d92b3679de845f42f0f\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n# Part 1\n\n# 3. Data Import & Preparation\n\n## Load Indego Trip Data (Q3 2024)\n\nDifferent quarters can be downloaded from: https://www.rideindego.com/about/data/\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q3 2024 data\nindego <- read_csv(\"data/indego-trips-2024-q3.csv\")\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408,408\nColumns: 15\n$ trip_id             <dbl> 953311130, 953308580, 953309085, 953334269, 953334…\n$ duration            <dbl> 5, 3, 2, 12, 22, 47, 111, 7, 1, 8, 6, 2, 20, 246, …\n$ start_time          <chr> \"7/1/2024 0:02\", \"7/1/2024 0:03\", \"7/1/2024 0:04\",…\n$ end_time            <chr> \"7/1/2024 0:07\", \"7/1/2024 0:06\", \"7/1/2024 0:06\",…\n$ start_station       <dbl> 3271, 3304, 3052, 3323, 3209, 3101, 3338, 3373, 33…\n$ start_lat           <dbl> 39.94760, 39.94234, 39.94732, 40.02796, 39.94900, …\n$ start_lon           <dbl> -75.22946, -75.15399, -75.15695, -75.22770, -75.21…\n$ end_station         <dbl> 3338, 3028, 3007, 3323, 3344, 3296, 3258, 3201, 33…\n$ end_lat             <dbl> 39.95098, 39.94061, 39.94517, 40.02796, 39.95961, …\n$ end_lon             <dbl> -75.23035, -75.14958, -75.15993, -75.22770, -75.23…\n$ bike_id             <chr> \"03439\", \"11905\", \"26031\", \"19971\", \"20921\", \"2241…\n$ plan_duration       <dbl> 30, 30, 30, 1, 30, 1, 30, 30, 30, 30, 30, 30, 30, …\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Day Pass\", \"I…\n$ bike_type           <chr> \"standard\", \"standard\", \"electric\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n**Discussion:** I chose quarter 3 because I had previously analyzed Winter 2025 (Q1) and wanted to explore Summer patterns as well. Looking at Q3 allows me to compare seasonal differences in Philadelphia’s ridership and understand how demand shifts between winter and summer.\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q3 2024:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q3 2024: 408408 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1719792120 to 1727740740 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 261 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    380939      27469 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Day Pass  Indego30 Indego365   Walk-up \n    22885    239857    132358     13308 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  236839   171569 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setlocale(\"LC_TIME\", \"C\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C\"\n```\n\n\n:::\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-07-01 00:02:00 2024-07-01 00:00:00    27 Mon       0       0\n2 2024-07-01 00:03:00 2024-07-01 00:00:00    27 Mon       0       0\n3 2024-07-01 00:04:00 2024-07-01 00:00:00    27 Mon       0       0\n4 2024-07-01 00:05:00 2024-07-01 00:00:00    27 Mon       0       0\n5 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n6 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# 4. Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2024\",\n    subtitle = \"summer demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Discussion:** During Summer 2024, ridership in Philadelphia stays consistently high, with strong weekday peaks and lower weekend dips. The smoothed trend shows a slight decline in early summer, followed by a steady increase through August and September before dropping again at the start of October.\n \n\n::: {.cell}\n\n```{.r .cell-code}\nfly_eagles_fly <- daily_trips %>% filter(date == \"2024-09-02\")\n\ntypical_boring_friday <- indego %>%\n  filter(dotw == \"Mon\", date != \"2024-09-02\") %>%\n  group_by(date) %>%\n  summarize(trips = n()) %>%\n  summarize(avg_Monday_trips = mean(trips))\n\nprint(fly_eagles_fly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  date       trips\n  <date>     <int>\n1 2024-09-02  3885\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(typical_boring_friday)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  avg_Monday_trips\n             <dbl>\n1            4481.\n```\n\n\n:::\n:::\n\n**Discussion:** September 2, 2024 was Labor Day in the United States, a major public holiday. Because Labor Day is a federal holiday, many people are off work, commute patterns drop sharply, and fewer riders use bike-share. As a result, daily ridership on September 2 is much lower than a typical Monday.\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns, Q3 2024\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Discussion:** Peak hours occur around 8 AM and 5 PM on weekdays, showing a classic work-commute pattern. On weekends, the peaks shift later in the day and are much flatter—riders use the system more for leisure, with steady midday activity instead of sharp morning and evening spikes.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,436 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 4,421 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 4,252 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 4,226 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 4,154 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 4,144 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,114 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 4,022 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,954 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,948 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,942 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 3,921 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 3,919 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,796 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,046 </td>\n   <td style=\"text-align:right;\"> 39.95012 </td>\n   <td style=\"text-align:right;\"> -75.14472 </td>\n   <td style=\"text-align:right;\"> 3,734 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,726 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,678 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# 5. Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n# 6. Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# 7. Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q3 2024: July 1 - September 30\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :55.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 7.000  \n Mean   :75.59   Mean   :0.007896   Mean   : 6.893  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.250000   Max.   :44.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly summer?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2024\",\n    subtitle = \"Summer to early fall transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n---\n\n# 8. Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 193072\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 241\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2202\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 530,682 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 193,072 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 337,610 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 530,682 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   :55.00   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.0000  \n Median : 0.0000   Median :76.00   Median :0.0000  \n Mean   : 0.7068   Mean   :75.59   Mean   :0.0079  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.0000  \n Max.   :24.0000   Max.   :98.00   Max.   :1.2500  \n                   NA's   :5784    NA's   :5784    \n```\n\n\n:::\n:::\n\n\n---\n\n# 9. Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 646,362 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n---\n\n# 10. Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q3 has weeks 27-40 (July-Spet)\n# Train on weeks 27-35 \n# Test on weeks 36-40 \n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 36)\n\ntest <- study_panel_complete %>%\n  filter(week >= 36)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 426,995 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 203,275 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19905 to 19967 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19968 to 19996 \n```\n\n\n:::\n:::\n\n\n---\n\n# 11. Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6699 -0.7865 -0.1952  0.1894 23.4789 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.1907758  0.0300331   6.352  0.00000000021248305 ***\nas.factor(hour)1  -0.0983522  0.0126388  -7.782  0.00000000000000717 ***\nas.factor(hour)2  -0.1241393  0.0128704  -9.645 < 0.0000000000000002 ***\nas.factor(hour)3  -0.1764085  0.0132525 -13.311 < 0.0000000000000002 ***\nas.factor(hour)4  -0.1584485  0.0130394 -12.152 < 0.0000000000000002 ***\nas.factor(hour)5  -0.0294027  0.0128895  -2.281              0.02254 *  \nas.factor(hour)6   0.2317631  0.0130472  17.763 < 0.0000000000000002 ***\nas.factor(hour)7   0.5137459  0.0130438  39.386 < 0.0000000000000002 ***\nas.factor(hour)8   0.9225191  0.0130186  70.862 < 0.0000000000000002 ***\nas.factor(hour)9   0.6732479  0.0131111  51.349 < 0.0000000000000002 ***\nas.factor(hour)10  0.5471686  0.0129073  42.392 < 0.0000000000000002 ***\nas.factor(hour)11  0.5675162  0.0130596  43.456 < 0.0000000000000002 ***\nas.factor(hour)12  0.6846655  0.0129836  52.733 < 0.0000000000000002 ***\nas.factor(hour)13  0.6714138  0.0127505  52.658 < 0.0000000000000002 ***\nas.factor(hour)14  0.7408376  0.0127923  57.913 < 0.0000000000000002 ***\nas.factor(hour)15  0.7774720  0.0127123  61.159 < 0.0000000000000002 ***\nas.factor(hour)16  0.9943089  0.0130734  76.056 < 0.0000000000000002 ***\nas.factor(hour)17  1.3409901  0.0131651 101.859 < 0.0000000000000002 ***\nas.factor(hour)18  1.0705918  0.0132411  80.854 < 0.0000000000000002 ***\nas.factor(hour)19  0.8329199  0.0130579  63.787 < 0.0000000000000002 ***\nas.factor(hour)20  0.6032729  0.0126386  47.733 < 0.0000000000000002 ***\nas.factor(hour)21  0.3788668  0.0127578  29.697 < 0.0000000000000002 ***\nas.factor(hour)22  0.2653238  0.0124655  21.285 < 0.0000000000000002 ***\nas.factor(hour)23  0.1487921  0.0132089  11.264 < 0.0000000000000002 ***\ndotw_simple2       0.0666909  0.0073090   9.125 < 0.0000000000000002 ***\ndotw_simple3       0.0674092  0.0071645   9.409 < 0.0000000000000002 ***\ndotw_simple4      -0.0019771  0.0071948  -0.275              0.78348    \ndotw_simple5      -0.0558390  0.0071752  -7.782  0.00000000000000714 ***\ndotw_simple6      -0.0528449  0.0072359  -7.303  0.00000000000028159 ***\ndotw_simple7      -0.1200020  0.0071570 -16.767 < 0.0000000000000002 ***\nTemperature        0.0007438  0.0003513   2.118              0.03421 *  \nPrecipitation     -0.0900337  0.0322306  -2.793              0.00522 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 426963 degrees of freedom\nMultiple R-squared:  0.1081,\tAdjusted R-squared:  0.1081 \nF-statistic:  1670 on 31 and 426963 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays have positive coefficients (0.029 to 0.052)\n- Tuesday has the highest weekday effect (+0.052)\n- Weekdays likely benefit from concentrated commuting patterns\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.061 and -0.065)\n- This means FEWER trips per station-hour than Monday\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1      -0.018       slightly fewer than midnight\n...\n6      +0.151       morning activity starting\n7      +0.276       morning rush building\n8      +0.487       PEAK morning rush\n9      +0.350       post-rush\n...\n17     +0.568       PEAK evening rush (5 PM!)\n18     +0.389       evening declining\n...\n23     +0.034       late night minimal\n\nIsn't this fun!\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8107 -0.4918 -0.1362  0.1938 20.2697 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.0786561  0.0260209   3.023             0.002505 ** \nas.factor(hour)1  -0.0305732  0.0109513  -2.792             0.005243 ** \nas.factor(hour)2  -0.0204266  0.0111573  -1.831             0.067134 .  \nas.factor(hour)3  -0.0442404  0.0114929  -3.849             0.000118 ***\nas.factor(hour)4  -0.0252562  0.0113182  -2.231             0.025650 *  \nas.factor(hour)5   0.0929684  0.0111892   8.309 < 0.0000000000000002 ***\nas.factor(hour)6   0.2902340  0.0113286  25.620 < 0.0000000000000002 ***\nas.factor(hour)7   0.4463363  0.0113331  39.384 < 0.0000000000000002 ***\nas.factor(hour)8   0.6894525  0.0113249  60.879 < 0.0000000000000002 ***\nas.factor(hour)9   0.3025745  0.0114123  26.513 < 0.0000000000000002 ***\nas.factor(hour)10  0.2552202  0.0112091  22.769 < 0.0000000000000002 ***\nas.factor(hour)11  0.2808766  0.0113442  24.759 < 0.0000000000000002 ***\nas.factor(hour)12  0.4037984  0.0112751  35.813 < 0.0000000000000002 ***\nas.factor(hour)13  0.3690166  0.0110804  33.303 < 0.0000000000000002 ***\nas.factor(hour)14  0.4126816  0.0111195  37.113 < 0.0000000000000002 ***\nas.factor(hour)15  0.4478829  0.0110488  40.537 < 0.0000000000000002 ***\nas.factor(hour)16  0.6138841  0.0113736  53.974 < 0.0000000000000002 ***\nas.factor(hour)17  0.8694958  0.0114777  75.755 < 0.0000000000000002 ***\nas.factor(hour)18  0.4952317  0.0115800  42.766 < 0.0000000000000002 ***\nas.factor(hour)19  0.3802954  0.0113812  33.414 < 0.0000000000000002 ***\nas.factor(hour)20  0.2327581  0.0110158  21.129 < 0.0000000000000002 ***\nas.factor(hour)21  0.1360156  0.0110822  12.273 < 0.0000000000000002 ***\nas.factor(hour)22  0.1435774  0.0108077  13.285 < 0.0000000000000002 ***\nas.factor(hour)23  0.0581483  0.0114470   5.080        0.00000037806 ***\ndotw_simple2       0.0211603  0.0063332   3.341             0.000834 ***\ndotw_simple3       0.0121472  0.0062096   1.956             0.050444 .  \ndotw_simple4      -0.0313699  0.0062374  -5.029        0.00000049236 ***\ndotw_simple5      -0.0342061  0.0062178  -5.501        0.00000003772 ***\ndotw_simple6      -0.0381694  0.0062715  -6.086        0.00000000116 ***\ndotw_simple7      -0.0578545  0.0062045  -9.325 < 0.0000000000000002 ***\nTemperature       -0.0011927  0.0003044  -3.918        0.00008929826 ***\nPrecipitation      0.0618332  0.0279337   2.214             0.026859 *  \nlag1Hour           0.3767247  0.0014350 262.520 < 0.0000000000000002 ***\nlag3Hours          0.1281318  0.0014104  90.845 < 0.0000000000000002 ***\nlag1day            0.1448192  0.0013178 109.891 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.046 on 426960 degrees of freedom\nMultiple R-squared:  0.3307,\tAdjusted R-squared:  0.3306 \nF-statistic:  6204 on 34 and 426960 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Adding lags improve R². Adding lags improves R² because past ridership strongly predicts current ridership. Bike use is highly autocorrelated—if trips were high one hour (or one day) ago, they are likely to be high now as well.\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2111 -0.7300 -0.2851  0.4535 20.1808 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7753069602  0.0569020384  13.625\nas.factor(hour)1          0.0336186991  0.0377901050   0.890\nas.factor(hour)2          0.0487073183  0.0413330957   1.178\nas.factor(hour)3         -0.0447495990  0.0512983936  -0.872\nas.factor(hour)4         -0.0853899954  0.0449166805  -1.901\nas.factor(hour)5          0.0090602737  0.0336006738   0.270\nas.factor(hour)6          0.2842169220  0.0300200985   9.468\nas.factor(hour)7          0.4337803415  0.0285613711  15.188\nas.factor(hour)8          0.7064995985  0.0276799079  25.524\nas.factor(hour)9          0.1844068018  0.0278801834   6.614\nas.factor(hour)10         0.1621109760  0.0278934198   5.812\nas.factor(hour)11         0.2192858261  0.0280222299   7.825\nas.factor(hour)12         0.3325947064  0.0276609321  12.024\nas.factor(hour)13         0.2872568947  0.0274476983  10.466\nas.factor(hour)14         0.3479704696  0.0273979816  12.701\nas.factor(hour)15         0.4017667237  0.0273268893  14.702\nas.factor(hour)16         0.5988989541  0.0274585814  21.811\nas.factor(hour)17         0.9434659166  0.0273617566  34.481\nas.factor(hour)18         0.4673445776  0.0276570683  16.898\nas.factor(hour)19         0.3343084390  0.0277357348  12.053\nas.factor(hour)20         0.1706327318  0.0277943019   6.139\nas.factor(hour)21         0.1102048135  0.0286974725   3.840\nas.factor(hour)22         0.1561084025  0.0289810638   5.387\nas.factor(hour)23         0.0782781732  0.0312446790   2.505\ndotw_simple2              0.0344509932  0.0126053537   2.733\ndotw_simple3              0.0322340427  0.0124474335   2.590\ndotw_simple4             -0.0390989926  0.0125576132  -3.114\ndotw_simple5             -0.0780794855  0.0125351420  -6.229\ndotw_simple6             -0.0194068626  0.0129583220  -1.498\ndotw_simple7             -0.0516221889  0.0128483055  -4.018\nTemperature               0.0008575906  0.0006077802   1.411\nPrecipitation            -0.3729418315  0.0699319873  -5.333\nlag1Hour                  0.2752540882  0.0021977530 125.243\nlag3Hours                 0.0856650838  0.0022698183  37.741\nlag1day                   0.1119405714  0.0021580117  51.872\nMed_Inc.x                 0.0000007633  0.0000001152   6.623\nPercent_Taking_Transit.y -0.0038729434  0.0004225528  -9.166\nPercent_White.y           0.0029655331  0.0002117857  14.003\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.373673    \nas.factor(hour)2                     0.238635    \nas.factor(hour)3                     0.383025    \nas.factor(hour)4                     0.057294 .  \nas.factor(hour)5                     0.787433    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9          0.00000000003746268 ***\nas.factor(hour)10         0.00000000619258551 ***\nas.factor(hour)11         0.00000000000000509 ***\nas.factor(hour)12        < 0.0000000000000002 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14        < 0.0000000000000002 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19        < 0.0000000000000002 ***\nas.factor(hour)20         0.00000000083178003 ***\nas.factor(hour)21                    0.000123 ***\nas.factor(hour)22         0.00000007192152554 ***\nas.factor(hour)23                    0.012235 *  \ndotw_simple2                         0.006276 ** \ndotw_simple3                         0.009609 ** \ndotw_simple4                         0.001849 ** \ndotw_simple5              0.00000000047108679 ***\ndotw_simple6                         0.134230    \ndotw_simple7              0.00005876669231488 ***\nTemperature                          0.158240    \nPrecipitation             0.00000009678053825 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                 0.00000000003529142 ***\nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.28 on 154100 degrees of freedom\n  (因为不存在，272857个观察量被删除了)\nMultiple R-squared:  0.2188,\tAdjusted R-squared:  0.2186 \nF-statistic:  1167 on 37 and 154100 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2481647 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2468552 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?** Baseline differences in demand across stations (some are just busier than others!).\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2530642 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2517486 \n```\n\n\n:::\n:::\n\n\n---\n\n# 12. Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.69 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.91 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Discussion:** Which features gave us the biggest improvement?\nTemporal Lags\n\n---\n\n# 13. Compare Results to  Q1 2025\n\n## MAE Comparison\n\n::: {.cell}\n\n```{.r .cell-code}\n## MAE comparison\nq1_mae <- c(0.60, 0.50, 0.74, 0.73, 0.73) ## Matrics from in-class exercise\n\nmae_compare <- data.frame(\n  Model  = mae_results$Model,\n  Q1_MAE = q1_mae,\n  Q3_MAE = mae_results$MAE   \n)\n\nkable(\n  mae_compare,\n  digits  = 2,\n  caption = \"Mean Absolute Error by Model: Q1 vs Q3 (Test Set)\",\n  col.names = c(\"Model\", \"Q1_MAE (trips)\", \"Q3_MAE (trips)\")\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model: Q1 vs Q3 (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> Q1_MAE (trips) </th>\n   <th style=\"text-align:right;\"> Q3_MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.69 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n   <td style=\"text-align:right;\"> 0.91 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Discussion:** Overall, Model 2 (model with Temporal Lags) has the lowest error in both Q1 and Q3. However, all models show higher errors in Q3, meaning that summer demand is harder to predict than winter demand. This suggests that temporal lags remain the strongest predictor across seasons, but Q3 likely needs additional features because bike usage is more variable in summer.\n\n## Temporal Patterns Comparison (summer vs. winter)\n\n::: {.cell}\n\n```{.r .cell-code}\n## Comparison between Q1 2025 and Q3 2024 (summer vs. winter)\nlibrary(ggplot2)\nlibrary(png)\nlibrary(grid)\nlibrary(cowplot)\n\nimg1 <- png::readPNG(\"data/daily_ridership_Q3.png\")\np_left <- grid::rasterGrob(img1, interpolate = TRUE)\n\nimg2 <- png::readPNG(\"data/daily_ridership_Q1.png\")  \np_right <- grid::rasterGrob(img2, interpolate = TRUE)\n\ncowplot::plot_grid(\n  p_left,\n  p_right,\n  labels = c(\"summer vs. winter\"),\n  ncol = 2\n)\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n**Discussion:** Summer and winter show clearly different temporal ridership patterns. In summer (Q3 2024), daily trips stay consistently high, with strong weekday–weekend fluctuations and a pattern that rises through mid-season before gradually declining toward October. In contrast, winter (Q1 2025) begins at much lower levels and shows a steady upward trend as temperatures warm, with ridership recovering from January lows and climbing sharply into March and April. Overall, summer displays high, stable, and highly active demand, while winter reflects low but steadily increasing activity, capturing a clear seasonal effect in bike-share usage.\n\n\n## Important Features\n\n::: {.cell}\n\n```{.r .cell-code}\n## Use partial R square to determine the  most important feature\nlibrary(rsq)\nrsq.partial(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$adjustment\n[1] FALSE\n\n$variable\n[1] \"as.factor(hour)\" \"dotw_simple\"     \"Temperature\"     \"Precipitation\"  \n[5] \"lag1Hour\"        \"lag3Hours\"       \"lag1day\"        \n\n$partial.rsq\n[1] 0.04465286552 0.00067301023 0.00003595236 0.00001147613 0.13897973178\n[6] 0.01896281475 0.02750562533\n```\n\n\n:::\n:::\n\n\n**Discussion:** In both Q1 and Q3, the one-hour lag (lag1Hour) is by far the most important predictor, followed by hour-of-day and the one-day lag. Weather and day-of-week contribute very little once temporal patterns and lags are included, and their independent explanatory power is even smaller in Q3.\n\n# Part 2\n\n# 14. Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Discussion:** The model performs best during low-demand periods—overnight and early morning—where observed trips are small and predictions stay close to the perfect-fit red line. It struggles the most during high-demand times like the AM and PM rush. In these periods, the model consistently underpredicts the highest trip volumes, as shown by the green line falling below the red line at larger values.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 0.8,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 0.8,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n:::\n\n\n**Question:** There is clear spatial clustering. The highest errors are concentrated in the high-demand areas of Philadelphia’s city center—especially around Center City and University City—where ridership is busiest and harder for the model to predict accurately.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](assignment5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Discussion**: Errors tend to be slightly higher in higher-income and higher-percent-White neighborhoods, while they are lower in areas with high transit usage. This suggests the model struggles more in wealthier, less transit-oriented areas. The equity implication is that model performance is not uniform—some communities may receive less accurate predictions than others, which could affect planning decisions if not addressed.\n\n---\n\n# Part 3\n\n# 15. Feature Engineering: New Features\n\n## Spatial features\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geosphere)\n\n## Distance to Center City\n\n# Center City coordinate (Use Philadelphia City Hall)\ncenter_lat <- 39.952800\ncenter_lon <- -75.163500\n\n# Calculate distance in kilometers\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    dist_to_center = distHaversine(\n      cbind(start_lon.y, start_lat.y),\n      cbind(center_lon, center_lat)\n    ) / 1000  # convert meters to km\n  )\n```\n:::\n\n\n## Trip history features\n\n::: {.cell}\n\n```{.r .cell-code}\n## Rolling 7-day average demand\nlibrary(slider)\n\nstudy_panel_complete <- study_panel_complete %>%\n  arrange(start_station, date, hour) %>%\n  group_by(start_station) %>%\n  mutate(\n    rolling7 = slide_dbl(\n      Trip_Count,\n      mean,\n      .before = 168,\n      .complete = TRUE\n    )\n  ) %>%\n  ungroup()\n```\n:::\n\n\n# 16. Fit the New Model\n\n## Split train/test Dataset Again\n\n::: {.cell}\n\n```{.r .cell-code}\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 36)\n\ntest <- study_panel_complete %>%\n  filter(week >= 36)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 426,995 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 203,275 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19905 to 19967 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19968 to 19996 \n```\n\n\n:::\n:::\n\n\n## Add two new presictors into the best model (previous model 2)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\nmodel_n <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + dist_to_center + rolling7,\n  data = train\n)\n\nsummary(model_n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + dist_to_center + \n    rolling7, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8100 -0.5124 -0.1265  0.2843 19.7776 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.1958752  0.0277630  -7.055     0.00000000000173 ***\nas.factor(hour)1  -0.0415843  0.0111714  -3.722             0.000197 ***\nas.factor(hour)2  -0.0372606  0.0114023  -3.268             0.001084 ** \nas.factor(hour)3  -0.0723215  0.0117795  -6.140     0.00000000082819 ***\nas.factor(hour)4  -0.0498010  0.0115731  -4.303     0.00001684165582 ***\nas.factor(hour)5   0.0746431  0.0114265   6.532     0.00000000006478 ***\nas.factor(hour)6   0.2835021  0.0116223  24.393 < 0.0000000000000002 ***\nas.factor(hour)7   0.4889729  0.0116331  42.033 < 0.0000000000000002 ***\nas.factor(hour)8   0.7777459  0.0115745  67.195 < 0.0000000000000002 ***\nas.factor(hour)9   0.3873448  0.0116420  33.271 < 0.0000000000000002 ***\nas.factor(hour)10  0.3274795  0.0114177  28.682 < 0.0000000000000002 ***\nas.factor(hour)11  0.3681197  0.0116251  31.666 < 0.0000000000000002 ***\nas.factor(hour)12  0.4800323  0.0115432  41.586 < 0.0000000000000002 ***\nas.factor(hour)13  0.4531911  0.0113242  40.020 < 0.0000000000000002 ***\nas.factor(hour)14  0.5020342  0.0113335  44.296 < 0.0000000000000002 ***\nas.factor(hour)15  0.5339758  0.0113301  47.129 < 0.0000000000000002 ***\nas.factor(hour)16  0.7258076  0.0116584  62.256 < 0.0000000000000002 ***\nas.factor(hour)17  1.0083577  0.0117782  85.612 < 0.0000000000000002 ***\nas.factor(hour)18  0.6265717  0.0119013  52.647 < 0.0000000000000002 ***\nas.factor(hour)19  0.4922334  0.0116659  42.194 < 0.0000000000000002 ***\nas.factor(hour)20  0.3215873  0.0112581  28.565 < 0.0000000000000002 ***\nas.factor(hour)21  0.2011924  0.0113167  17.778 < 0.0000000000000002 ***\nas.factor(hour)22  0.1693394  0.0110102  15.380 < 0.0000000000000002 ***\nas.factor(hour)23  0.0818889  0.0117281   6.982     0.00000000000291 ***\ndotw_simple2       0.0400605  0.0064564   6.205     0.00000000054841 ***\ndotw_simple3       0.0373379  0.0063044   5.922     0.00000000317380 ***\ndotw_simple4      -0.0109365  0.0063431  -1.724             0.084681 .  \ndotw_simple5      -0.0352449  0.0063550  -5.546     0.00000002925128 ***\ndotw_simple6      -0.0436666  0.0064038  -6.819     0.00000000000919 ***\ndotw_simple7      -0.0722071  0.0063441 -11.382 < 0.0000000000000002 ***\nTemperature       -0.0011194  0.0003213  -3.484             0.000494 ***\nPrecipitation      0.0493853  0.0272746   1.811             0.070192 .  \nlag1Hour           0.3254434  0.0015337 212.196 < 0.0000000000000002 ***\nlag3Hours          0.0579561  0.0015231  38.050 < 0.0000000000000002 ***\nlag1day            0.0723473  0.0014279  50.666 < 0.0000000000000002 ***\ndist_to_center    -0.0058476  0.0012056  -4.850     0.00000123328852 ***\nrolling7           0.5169363  0.0045691 113.137 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.018 on 387478 degrees of freedom\n  (因为不存在，39480个观察量被删除了)\nMultiple R-squared:  0.3686,\tAdjusted R-squared:  0.3685 \nF-statistic:  6283 on 36 and 387478 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n# 17. New Model Evaluation and MAE\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred_n = predict(model_n, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results_new <- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"New Model. + Rolling 7-day average demand and Distance to Center City\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred_n), na.rm = TRUE)\n  ),\n   RMSE = c(\n    sqrt(mean((test$Trip_Count - test$pred2)^2,  na.rm = TRUE)),\n    sqrt(mean((test$Trip_Count - test$pred_n)^2, na.rm = TRUE))\n  )\n)\n\nkable(mae_results_new, \n      digits = 5,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\", \"RMSE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n   <th style=\"text-align:right;\"> RMSE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> NaN </td>\n   <td style=\"text-align:right;\"> NaN </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> New Model. + Rolling 7-day average demand and Distance to Center City </td>\n   <td style=\"text-align:right;\"> 0.69424 </td>\n   <td style=\"text-align:right;\"> 1.05677 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Discussion:** Adding the two new features has very little impact on performance.\nCompared with the lag-only model, MAE increases slightly, meaning average errors get a bit worse, while RMSE decreases slightly, meaning the model fits large errors a bit better. Overall, the changes are tiny, so rolling 7-day demand and distance to Center City add only limited predictive power beyond the temporal lags.\n\n# 18.  Attempt of poisson model\n\n::: {.cell}\n\n```{.r .cell-code}\n## Train a poisson model\nmodel_poisson <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + dist_to_center + rolling7,\n  family = poisson(link = \"log\"),\n  data   = train\n)\n\nsummary(model_poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + dist_to_center + \n    rolling7, family = poisson(link = \"log\"), data = train)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(>|z|)    \n(Intercept)       -1.6300179  0.0340824 -47.826 < 0.0000000000000002 ***\nas.factor(hour)1  -0.4801013  0.0263496 -18.220 < 0.0000000000000002 ***\nas.factor(hour)2  -0.6423562  0.0287214 -22.365 < 0.0000000000000002 ***\nas.factor(hour)3  -1.2789145  0.0382654 -33.422 < 0.0000000000000002 ***\nas.factor(hour)4  -1.0671358  0.0343024 -31.110 < 0.0000000000000002 ***\nas.factor(hour)5  -0.0025455  0.0238758  -0.107             0.915095    \nas.factor(hour)6   0.7653694  0.0204428  37.440 < 0.0000000000000002 ***\nas.factor(hour)7   1.1963036  0.0190756  62.714 < 0.0000000000000002 ***\nas.factor(hour)8   1.5268545  0.0182915  83.474 < 0.0000000000000002 ***\nas.factor(hour)9   1.1724467  0.0187602  62.497 < 0.0000000000000002 ***\nas.factor(hour)10  1.0823756  0.0189237  57.197 < 0.0000000000000002 ***\nas.factor(hour)11  1.1262508  0.0190026  59.268 < 0.0000000000000002 ***\nas.factor(hour)12  1.2539842  0.0186636  67.189 < 0.0000000000000002 ***\nas.factor(hour)13  1.2312519  0.0185416  66.405 < 0.0000000000000002 ***\nas.factor(hour)14  1.2885874  0.0184336  69.904 < 0.0000000000000002 ***\nas.factor(hour)15  1.3205147  0.0183929  71.795 < 0.0000000000000002 ***\nas.factor(hour)16  1.4920816  0.0182564  81.729 < 0.0000000000000002 ***\nas.factor(hour)17  1.6683403  0.0180373  92.494 < 0.0000000000000002 ***\nas.factor(hour)18  1.3678219  0.0184288  74.222 < 0.0000000000000002 ***\nas.factor(hour)19  1.2660181  0.0185584  68.218 < 0.0000000000000002 ***\nas.factor(hour)20  1.0727909  0.0187843  57.111 < 0.0000000000000002 ***\nas.factor(hour)21  0.8402675  0.0194505  43.200 < 0.0000000000000002 ***\nas.factor(hour)22  0.6875229  0.0197041  34.892 < 0.0000000000000002 ***\nas.factor(hour)23  0.4554077  0.0214480  21.233 < 0.0000000000000002 ***\ndotw_simple2       0.0523519  0.0072783   7.193    0.000000000000634 ***\ndotw_simple3       0.0480459  0.0071654   6.705    0.000000000020105 ***\ndotw_simple4      -0.0078726  0.0073120  -1.077             0.281629    \ndotw_simple5      -0.0685598  0.0074105  -9.252 < 0.0000000000000002 ***\ndotw_simple6      -0.0676553  0.0076248  -8.873 < 0.0000000000000002 ***\ndotw_simple7      -0.1193391  0.0076162 -15.669 < 0.0000000000000002 ***\nTemperature       -0.0013827  0.0003581  -3.861             0.000113 ***\nPrecipitation     -0.0622825  0.0418879  -1.487             0.137045    \nlag1Hour           0.1408461  0.0010218 137.845 < 0.0000000000000002 ***\nlag3Hours          0.0278361  0.0012249  22.725 < 0.0000000000000002 ***\nlag1day            0.0316000  0.0011255  28.076 < 0.0000000000000002 ***\ndist_to_center    -0.1479677  0.0018021 -82.108 < 0.0000000000000002 ***\nrolling7           0.5959551  0.0043503 136.993 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 672205  on 387514  degrees of freedom\nResidual deviance: 405302  on 387478  degrees of freedom\n  (因为不存在，39480个观察量被删除了)\nAIC: 746699\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate dispersion parameter\ndispersion <- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDispersion parameter: 1.28 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Rule of thumb: >1.5 suggests overdispersion\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRule of thumb: >1.5 suggests overdispersion\n```\n\n\n:::\n\n```{.r .cell-code}\nif (dispersion > 1.5) {\n  cat(\"⚠ Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"✓ Dispersion looks okay for Poisson model.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Dispersion looks okay for Poisson model.\n```\n\n\n:::\n:::\n\n\n# 19. Evaluation of Poisson Model (MAE and RMSE)\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    pred_pois = predict(model_poisson, newdata = test, type = \"response\")\n  )\n\n# Calculate MAE for each model\nmae_results_nn <- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"New Model. + Rolling 7-day average demand and Distance to Center City\",\n    \"New Poisson Model\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred_n), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred_pois), na.rm = TRUE)\n  ),\n   RMSE = c(\n    sqrt(mean((test$Trip_Count - test$pred2)^2,  na.rm = TRUE)),\n    sqrt(mean((test$Trip_Count - test$pred_n)^2, na.rm = TRUE)),\n    sqrt(mean((test$Trip_Count - test$pred_pois)^2, na.rm = TRUE))\n  )\n)\n\nkable(mae_results_nn, \n      digits = 5,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\", \"RMSE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n   <th style=\"text-align:right;\"> RMSE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> NaN </td>\n   <td style=\"text-align:right;\"> NaN </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> New Model. + Rolling 7-day average demand and Distance to Center City </td>\n   <td style=\"text-align:right;\"> 0.69424 </td>\n   <td style=\"text-align:right;\"> 1.05677 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> New Poisson Model </td>\n   <td style=\"text-align:right;\"> 0.70353 </td>\n   <td style=\"text-align:right;\"> 1.31661 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Discussion:** The Poisson model performs worse: both MAE and RMSE are higher than the lag-based linear models. \n\n## Part 4: Critical Reflection \n\nOur final MAE is moderate, which is still relatively large that Indego should be cautious about using this model for real-time rebalancing. The model often under-predicts during the highest demand periods, especially in the city center, so errors could translate directly into empty docks or no bikes when they are needed most. I would see this system as a useful decision-support tool for planning and for setting broad rebalancing priorities, but not as the only input for hourly truck routes. In practice, it should be combined with operator experience and live monitoring. The model also misses some important patterns: it cannot fully capture special events, holidays, or sudden weather changes, and it assumes that the relationships between predictors and demand stay stable over time (temporal stability). With more time and data, I would improve it by adding richer temporal and spatial features (e.g., event calendars, more detailed land-use and transit connections, longer history of ridership) and testing more flexible models that can capture nonlinear and complex effects.\n\nFrom an equity perspective, the error maps show that the largest errors occur in the highest-demand downtown neighborhoods, meaning the model is least accurate precisely where many riders rely on the system. If these biases are not addressed, the model could unintentionally worsen disparities—for example, by systematically under-serving busy core areas or misallocating bikes away from certain communities when predictions are wrong. To mitigate this, I would recommend regular fairness audits of prediction errors by neighborhood and demographic group, adding explicit equity targets or minimum-service constraints to any rebalancing strategy, and involving community feedback when evaluating model performance. This way, the system can support more efficient operations without sacrificing equitable access to bikes.\n\n---\n\n# Submission Requirements\n\n## What to Submit (per team)\n\n1. **Rmd file** with all your code (commented!)\n2. **HTML output** with results and visualizations\n3. **Brief report** summarizing (with supporting data & visualization):\n\n   - Your quarter and why you chose it\n   - Model comparison results\n   - Error analysis insights\n   - New features you added and why\n   - Critical reflection on deployment\n---",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}